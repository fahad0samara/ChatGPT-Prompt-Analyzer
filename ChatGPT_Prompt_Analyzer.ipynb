{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT Prompt Analyzer ðŸ¤–\n",
    "\n",
    "This notebook demonstrates the functionality of our ChatGPT Prompt Analyzer. It includes:\n",
    "- Feature extraction\n",
    "- Pattern matching\n",
    "- Classification\n",
    "- Visualization\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text\"\"\"\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', str(text))\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "def extract_features(text):\n",
    "    \"\"\"Extract features from text for classification\"\"\"\n",
    "    words = text.split()\n",
    "    first_three_words = ' '.join(words[:3])\n",
    "    \n    # Basic metrics\n",
    "    features = {\n",
    "        'word_count': len(words),\n",
    "        'avg_word_length': sum(len(word) for word in words) / len(words) if words else 0,\n",
    "        'contains_question': '?' in text,\n",
    "        'contains_exclamation': '!' in text,\n",
    "        'sentence_count': len(re.split(r'[.!?]+', text))\n",
    "    }\n",
    "    \n    # Add all other features from model_training.py\n",
    "    # ... (copy the rest of the feature extraction code)\n",
    "    \n    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def predict_type_with_confidence(text):\n",
    "    \"\"\"Predict interaction type and confidence scores\"\"\"\n",
    "    cleaned_text = clean_text(text)\n",
    "    features = extract_features(cleaned_text)\n",
    "    \n    scores = {\n",
    "        'analysis': (\n",
    "            features['analysis_pattern_score'] * 2.0 +\n",
    "            features['analysis_verb_score'] * 1.5 +\n",
    "            features['data_term_score'] +\n",
    "            features['quality_term_score'] +\n",
    "            features['context_term_score'] +\n",
    "            sum([\n",
    "                features['data_analysis_score'],\n",
    "                features['performance_analysis_score'],\n",
    "                features['quality_analysis_score'],\n",
    "                features['security_analysis_score']\n",
    "            ])\n",
    "        ),\n",
    "        'teaching': (\n",
    "            features['teaching_score'] * 1.5 +\n",
    "            (2.0 if features['starts_with_teaching'] else 0) +\n",
    "            (1.0 if features['has_basic_terms'] else 0)\n",
    "        ),\n",
    "        'guidance': (\n",
    "            features['guidance_score'] * 1.5 +\n",
    "            (2.0 if features['starts_with_guidance'] else 0)\n",
    "        ),\n",
    "        'creation': (\n",
    "            features['creation_score'] * 1.5 +\n",
    "            (2.0 if features['starts_with_creation'] else 0)\n",
    "        )\n",
    "    }\n",
    "    \n    # Normalize scores\n",
    "    total = sum(scores.values()) or 1\n",
    "    confidence_scores = {k: (v/total)*100 for k, v in scores.items()}\n",
    "    \n    # Get prediction\n",
    "    max_score = max(scores.values())\n",
    "    prediction = max(scores.items(), key=lambda x: x[1])[0] if max_score >= 1.0 else 'other'\n",
    "    \n    return prediction, confidence_scores, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_confidence_scores(confidence_scores):\n",
    "    \"\"\"Plot confidence scores using a bar chart\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    categories = list(confidence_scores.keys())\n",
    "    scores = list(confidence_scores.values())\n",
    "    \n    colors = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c']\n",
    "    plt.bar(categories, scores, color=colors)\n",
    "    plt.title('Confidence Scores by Category')\n",
    "    plt.ylabel('Confidence (%)')\n",
    "    plt.ylim(0, 100)\n",
    "    \n    for i, score in enumerate(scores):\n",
    "        plt.text(i, score + 1, f'{score:.1f}%', ha='center')\n",
    "    \n    plt.show()\n",
    "\n",
    "def plot_feature_analysis(features):\n",
    "    \"\"\"Plot detailed feature analysis\"\"\"\n",
    "    analysis_features = {\n",
    "        'Data Analysis': features['data_analysis_score'],\n",
    "        'Performance': features['performance_analysis_score'],\n",
    "        'Quality': features['quality_analysis_score'],\n",
    "        'Security': features['security_analysis_score']\n",
    "    }\n",
    "    \n    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(analysis_features.keys(), analysis_features.values())\n",
    "    plt.title('Analysis Feature Scores')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n    for i, score in enumerate(analysis_features.values()):\n",
    "        plt.text(i, score + 0.01, f'{score:.2f}', ha='center')\n",
    "    \n    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_prompt(prompt):\n",
    "    \"\"\"Analyze a prompt and display results\"\"\"\n",
    "    prediction, confidence_scores, features = predict_type_with_confidence(prompt)\n",
    "    \n    print(f\"ðŸŽ¯ Prediction: {prediction.title()}\\n\")\n",
    "    \n    print(\"ðŸ“Š Confidence Scores:\")\n",
    "    for category, score in sorted(confidence_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{category.title()}: {score:.1f}%\")\n",
    "    \n    print(\"\\nðŸ“ˆ Key Metrics:\")\n",
    "    print(f\"Word Count: {features['word_count']}\")\n",
    "    print(f\"Average Word Length: {features['avg_word_length']:.1f}\")\n",
    "    print(f\"Technical Term Ratio: {features['technical_term_ratio']:.2f}\")\n",
    "    \n    # Plot visualizations\n",
    "    plot_confidence_scores(confidence_scores)\n",
    "    plot_feature_analysis(features)\n",
    "    \n    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Try analyzing different prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example prompts\n",
    "example_prompts = [\n",
    "    \"Analyze this dataset and identify trends\",\n",
    "    \"Can you analyze the performance of this algorithm?\",\n",
    "    \"Please analyze our security vulnerabilities\",\n",
    "    \"Analyze the code quality of this module\"\n",
    "]\n",
    "\n",
    "# Analyze first example\n",
    "print(\"Example Analysis:\")\n",
    "analyze_prompt(example_prompts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Analysis\n",
    "\n",
    "Enter your own prompt to analyze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Your prompt here\n",
    "your_prompt = \"Enter your prompt here\"\n",
    "analyze_prompt(your_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
